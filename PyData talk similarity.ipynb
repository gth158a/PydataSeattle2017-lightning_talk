{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ! python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='images/intro.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='images/slide2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='images/slide3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='images/slide4a.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of talks: 65\n"
     ]
    }
   ],
   "source": [
    "page = \"https://pydata.org/seattle2017/schedule/\"\n",
    "html = urlopen(page)\n",
    "soup = BeautifulSoup(html.read(), \"lxml\")\n",
    "links = soup.find_all(\"span\", { \"class\" : \"title\" })\n",
    "print(\"Number of talks: \" + str(len(links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"title\">\n",
       "<a href=\"/seattle2017/schedule/presentation/57/\">Using CNTK's Python Interface for Deep Learning</a>\n",
       "</span>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<span class=\"title\">\n",
       "<a href=\"/seattle2017/schedule/presentation/113/\">Robust Algorithms for Machine Learning</a>\n",
       "</span>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using CNTK's Python Interface for Deep Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets get the title name and the hyperlink of one link\n",
    "test = links[0]\n",
    "print(test.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Using CNTK's Python Interface for Deep Learning\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_text().strip(' \\t\\n\\r') # trims spaces of title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/seattle2017/schedule/presentation/57/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = test.find(\"a\", href=True)\n",
    "parsed.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's get all titles and links to each of the talks\n",
    "titles = []\n",
    "page_link = []\n",
    "for link in links:\n",
    "    title = link.get_text().strip(' \\t\\n\\r') # trims spaces of link description\n",
    "    link = link.find(\"a\", href=True).attrs['href']\n",
    "    titles.append(title)\n",
    "    page_link.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Using CNTK's Python Interface for Deep Learning\",\n",
       " 'D‚Äôoh! Unevenly spaced time series analysis of The Simpsons in Pandas',\n",
       " 'From Novice to Data Ninja',\n",
       " 'So you want to be a Python expert?',\n",
       " 'Introduction to data analytics with pandas']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/seattle2017/schedule/presentation/57/',\n",
       " '/seattle2017/schedule/presentation/104/',\n",
       " '/seattle2017/schedule/presentation/109/',\n",
       " '/seattle2017/schedule/presentation/125/',\n",
       " '/seattle2017/schedule/presentation/105/']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_link[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Links and titles parsed üòú"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's scrape - Talk abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base = 'https://pydata.org'\n",
    "page2 = base + page_link[0]\n",
    "html2 = urlopen(page2)\n",
    "soup2 = BeautifulSoup(html2.read(), \"lxml\")\n",
    "abstract_description = soup2.find_all(\"div\", { \"class\" : \"abstract\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "abs_texts = []\n",
    "for link in abstract_description:\n",
    "    abstract_text = link.get_text().strip(' \\t\\n\\r') # trims spaces of link description\n",
    "    abs_texts.append(abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topics to be covered include ...\\n\\nCognitive Toolkit (CNTK) installation\\nWhat is \"machine learning\"? [gradient descent example]\\nWhat is \"learning representations\"?\\nWhy do Graphics Processing Units (GPUs) help?\\nHow do we prevent overfitting?\\nCNTK Packages and Modules\\nDeep Learning Examples [including Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) examples]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lets get all of our data üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "abs_texts = []\n",
    "for p in page_link:\n",
    "    page2 = base + p\n",
    "    html2 = urlopen(page2)\n",
    "    soup2 = BeautifulSoup(html2.read(), \"lxml\")\n",
    "    abstract_description = soup2.find_all(\"div\", { \"class\" : \"abstract\" })\n",
    "    \n",
    "    for link in abstract_description:\n",
    "        abstract_text = link.get_text().strip(' \\t\\n\\r') # trims spaces of link description\n",
    "        abstract_text = abstract_text.replace(\"\\n\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"?\",\"\").replace(\"]\",\"\").replace(\"[\",\"\")\n",
    "        abs_texts.append(abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PixieDust is a new Python open source library that helps data scientists and developers working in Jupyter Notebooks and Apache Spark to be more efficient. PixieDust speeds up data manipulation and display with features like:  Automated local install of Python and Scala kernels running with Spark  Realtime Spark Job progress monitoring directly from the Notebook  Use Scala directly in your Python notebook. Variables are automatically transferred from Python to Scala and vice-versa   Auto-visualisation of Spark DataFrames using popular chart engines like Matplotlib, Seaborn, Bokeh, or MapBox   Seamless integration to cloud services  Create embedded apps with your own visualisations or apps using the PixieDust extensibility APIs Come along and learn how you can use this tool in your own projects to visualise and explore data effortlessly with no coding. If you prefer working with a Scala Notebook, this session is also for you, as PixieDust can also run on a Scala Kernel. Imagine being able to visualise your favourite Python chart engines from a Scala Notebook! This session will end with a demo combining Twitter, Watson Tone Analyser, Spark Streaming, and some fun real-time visualisations - all running within a Notebook.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_texts[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PixieDust - make Jupyter Python Notebooks with Apache Spark Faster, Flexible, and Easier to use'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# talk # 20\n",
    "titles[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PixieDust is a new Python open source library that helps data scientists and developers working in Jupyter Notebooks and Apache Spark to be more efficient. PixieDust speeds up data manipulation and display with features like:  Automated local install of Python and Scala kernels running with Spark  Realtime Spark Job progress monitoring directly from the Notebook  Use Scala directly in your Python notebook. Variables are automatically transferred from Python to Scala and vice-versa   Auto-visualisation of Spark DataFrames using popular chart engines like Matplotlib, Seaborn, Bokeh, or MapBox   Seamless integration to cloud services  Create embedded apps with your own visualisations or apps using the PixieDust extensibility APIs Come along and learn how you can use this tool in your own projects to visualise and explore data effortlessly with no coding. If you prefer working with a Scala Notebook, this session is also for you, as PixieDust can also run on a Scala Kernel. Imagine being able to visualise your favourite Python chart engines from a Scala Notebook! This session will end with a demo combining Twitter, Watson Tone Analyser, Spark Streaming, and some fun real-time visualisations - all running within a Notebook.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_texts[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Spacy for tokenization and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_spacy = en_nlp(abs_texts[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pixiedust', 'is', 'a', 'new', 'python', 'open', 'source', 'library', 'that', 'helps', 'data', 'scientists', 'and', 'developers', 'working', 'in', 'jupyter', 'notebooks', 'and', 'apache', 'spark', 'to', 'be', 'more', 'efficient', '.', 'pixiedust', 'speeds', 'up', 'data', 'manipulation', 'and', 'display', 'with', 'features', 'like', ':', ' ', 'automated', 'local', 'install', 'of', 'python', 'and', 'scala', 'kernels', 'running', 'with', 'spark', ' ', 'realtime', 'spark', 'job', 'progress', 'monitoring', 'directly', 'from', 'the', 'notebook', ' ', 'use', 'scala', 'directly', 'in', 'your', 'python', 'notebook', '.', 'variables', 'are', 'automatically', 'transferred', 'from', 'python', 'to', 'scala', 'and', 'vice', '-', 'versa', '  ', 'auto', '-', 'visualisation', 'of', 'spark', 'dataframes', 'using', 'popular', 'chart', 'engines', 'like', 'matplotlib', ',', 'seaborn', ',', 'bokeh', ',', 'or', 'mapbox', '  ', 'seamless', 'integration', 'to', 'cloud', 'services', ' ', 'create', 'embedded', 'apps', 'with', 'your', 'own', 'visualisations', 'or', 'apps', 'using', 'the', 'pixiedust', 'extensibility', 'apis', 'come', 'along', 'and', 'learn', 'how', 'you', 'can', 'use', 'this', 'tool', 'in', 'your', 'own', 'projects', 'to', 'visualise', 'and', 'explore', 'data', 'effortlessly', 'with', 'no', 'coding', '.', 'if', 'you', 'prefer', 'working', 'with', 'a', 'scala', 'notebook', ',', 'this', 'session', 'is', 'also', 'for', 'you', ',', 'as', 'pixiedust', 'can', 'also', 'run', 'on', 'a', 'scala', 'kernel', '.', 'imagine', 'being', 'able', 'to', 'visualise', 'your', 'favourite', 'python', 'chart', 'engines', 'from', 'a', 'scala', 'notebook', '!', 'this', 'session', 'will', 'end', 'with', 'a', 'demo', 'combining', 'twitter', ',', 'watson', 'tone', 'analyser', ',', 'spark', 'streaming', ',', 'and', 'some', 'fun', 'real', '-', 'time', 'visualisations', '-', 'all', 'running', 'within', 'a', 'notebook', '.']\n"
     ]
    }
   ],
   "source": [
    "# simple tokenizer\n",
    "print([token.lower_ for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "['pixiedust', 'be', 'a', 'new', 'python', 'open', 'source', 'library', 'that', 'help', 'data', 'scientist', 'and', 'developer', 'work', 'in', 'jupyter', 'notebooks', 'and', 'apache', 'spark', 'to', 'be', 'more', 'efficient', '.', 'pixiedust', 'speed', 'up', 'datum', 'manipulation', 'and', 'display', 'with', 'feature', 'like', ':', ' ', 'automate', 'local', 'install', 'of', 'python', 'and', 'scala', 'kernel', 'run', 'with', 'spark', ' ', 'realtime', 'spark', 'job', 'progress', 'monitor', 'directly', 'from', 'the', 'notebook', ' ', 'use', 'scala', 'directly', 'in', '-PRON-', 'python', 'notebook', '.', 'variable', 'be', 'automatically', 'transfer', 'from', 'python', 'to', 'scala', 'and', 'vice', '-', 'versa', '  ', 'auto', '-', 'visualisation', 'of', 'spark', 'dataframes', 'use', 'popular', 'chart', 'engine', 'like', 'matplotlib', ',', 'seaborn', ',', 'bokeh', ',', 'or', 'mapbox', '  ', 'seamless', 'integration', 'to', 'cloud', 'service', ' ', 'create', 'embed', 'apps', 'with', '-PRON-', 'own', 'visualisation', 'or', 'app', 'use', 'the', 'pixiedust', 'extensibility', 'api', 'come', 'along', 'and', 'learn', 'how', '-PRON-', 'can', 'use', 'this', 'tool', 'in', '-PRON-', 'own', 'project', 'to', 'visualise', 'and', 'explore', 'datum', 'effortlessly', 'with', 'no', 'coding', '.', 'if', '-PRON-', 'prefer', 'work', 'with', 'a', 'scala', 'notebook', ',', 'this', 'session', 'be', 'also', 'for', '-PRON-', ',', 'as', 'pixiedust', 'can', 'also', 'run', 'on', 'a', 'scala', 'kernel', '.', 'imagine', 'be', 'able', 'to', 'visualise', '-PRON-', 'favourite', 'python', 'chart', 'engine', 'from', 'a', 'scala', 'notebook', '!', 'this', 'session', 'will', 'end', 'with', 'a', 'demo', 'combine', 'twitter', ',', 'watson', 'tone', 'analyser', ',', 'spark', 'streaming', ',', 'and', 'some', 'fun', 'real', '-', 'time', 'visualisation', '-', 'all', 'run', 'within', 'a', 'notebook', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "print(\"Lemmatization:\")\n",
    "print([token.lemma_ for token in doc_spacy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STOPLIST = [\"‚Äôs\", \"‚Äôre\", \"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS)\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"‚Äì\", \"-----\", \"---\", \"...\", \"‚Äú\", \"‚Äù\", \"'ve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = en_nlp(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove additional characters\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \"‚Äô\" in tokens:\n",
    "        tokens.remove(\"‚Äô\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"‚ÅÉ\" in tokens:\n",
    "        tokens.remove(\"‚ÅÉ\")\n",
    "    while \"‚Äî\" in tokens:\n",
    "        tokens.remove(\"‚Äî\")\n",
    "    while \"‚Ä¢\" in tokens:\n",
    "        tokens.remove(\"‚Ä¢\")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pixiedust',\n",
       " 'new',\n",
       " 'python',\n",
       " 'open',\n",
       " 'source',\n",
       " 'library',\n",
       " 'help',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'developer',\n",
       " 'work',\n",
       " 'jupyter',\n",
       " 'notebooks',\n",
       " 'apache',\n",
       " 'spark',\n",
       " 'efficient',\n",
       " 'pixiedust',\n",
       " 'speed',\n",
       " 'datum',\n",
       " 'manipulation',\n",
       " 'display',\n",
       " 'feature',\n",
       " 'like',\n",
       " 'automate',\n",
       " 'local',\n",
       " 'install',\n",
       " 'python',\n",
       " 'scala',\n",
       " 'kernel',\n",
       " 'run',\n",
       " 'spark',\n",
       " 'realtime',\n",
       " 'spark',\n",
       " 'job',\n",
       " 'progress',\n",
       " 'monitor',\n",
       " 'directly',\n",
       " 'notebook',\n",
       " 'use',\n",
       " 'scala',\n",
       " 'directly',\n",
       " 'python',\n",
       " 'notebook',\n",
       " 'variable',\n",
       " 'automatically',\n",
       " 'transfer',\n",
       " 'python',\n",
       " 'scala',\n",
       " 'vice',\n",
       " 'versa',\n",
       " 'auto',\n",
       " 'visualisation',\n",
       " 'spark',\n",
       " 'dataframes',\n",
       " 'use',\n",
       " 'popular',\n",
       " 'chart',\n",
       " 'engine',\n",
       " 'like',\n",
       " 'matplotlib',\n",
       " 'seaborn',\n",
       " 'bokeh',\n",
       " 'mapbox',\n",
       " 'seamless',\n",
       " 'integration',\n",
       " 'cloud',\n",
       " 'service',\n",
       " 'create',\n",
       " 'embed',\n",
       " 'apps',\n",
       " 'visualisation',\n",
       " 'app',\n",
       " 'use',\n",
       " 'pixiedust',\n",
       " 'extensibility',\n",
       " 'api',\n",
       " 'come',\n",
       " 'learn',\n",
       " 'use',\n",
       " 'tool',\n",
       " 'project',\n",
       " 'visualise',\n",
       " 'explore',\n",
       " 'datum',\n",
       " 'effortlessly',\n",
       " 'coding',\n",
       " 'prefer',\n",
       " 'work',\n",
       " 'scala',\n",
       " 'notebook',\n",
       " 'session',\n",
       " 'pixiedust',\n",
       " 'run',\n",
       " 'scala',\n",
       " 'kernel',\n",
       " 'imagine',\n",
       " 'able',\n",
       " 'visualise',\n",
       " 'favourite',\n",
       " 'python',\n",
       " 'chart',\n",
       " 'engine',\n",
       " 'scala',\n",
       " 'notebook',\n",
       " 'session',\n",
       " 'end',\n",
       " 'demo',\n",
       " 'combine',\n",
       " 'twitter',\n",
       " 'watson',\n",
       " 'tone',\n",
       " 'analyser',\n",
       " 'spark',\n",
       " 'streaming',\n",
       " 'fun',\n",
       " 'real',\n",
       " 'time',\n",
       " 'visualisation',\n",
       " 'run',\n",
       " 'notebook']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = tokenizeText(abs_texts[20])\n",
    "sample1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_all = TfidfVectorizer(tokenizer=tokenizeText, stop_words=STOPLIST, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf_idf_all.fit_transform(abs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<65x1773 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4110 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = pd.DataFrame(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 1773)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1763</th>\n",
       "      <th>1764</th>\n",
       "      <th>1765</th>\n",
       "      <th>1766</th>\n",
       "      <th>1767</th>\n",
       "      <th>1768</th>\n",
       "      <th>1769</th>\n",
       "      <th>1770</th>\n",
       "      <th>1771</th>\n",
       "      <th>1772</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 1773 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   1763  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
       "\n",
       "   1764  1765  1766  1767  1768  1769  1770  1771  1772  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1773 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='images/slide5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To compensate for the effect of document length, the standard way of quantifying the similarity between two documents $d_1$ and $d_2$ is to compute the cosine similarity of their vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize each talk to simplifies 'cosine similarity' to dot-product of two vectors\n",
    "docs_normed = normalize(docs,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summed = docs.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1773,)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 386, 1685, 1005,  376, 1268,  886,  200, 1576, 1245, 1613, 1056,\n",
       "       1041,  540,  617,   81,  932,  256,  940,  400,  895])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = np.argsort(-summed)[:20]\n",
    "max_words.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386     3.942727\n",
       "1685    3.269268\n",
       "1005    2.764599\n",
       "376     2.210588\n",
       "1268    2.202805\n",
       "886     2.048558\n",
       "200     2.037208\n",
       "1576    1.655281\n",
       "1245    1.641570\n",
       "1613    1.631103\n",
       "1056    1.549369\n",
       "1041    1.474834\n",
       "540     1.435485\n",
       "617     1.368837\n",
       "81      1.338632\n",
       "932     1.337837\n",
       "256     1.313613\n",
       "940     1.312429\n",
       "400     1.296345\n",
       "895     1.295913\n",
       "dtype: float64"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed[max_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datum\n",
      "use\n",
      "model\n",
      "data\n",
      "python\n",
      "learn\n",
      "build\n",
      "talk\n",
      "provide\n",
      "time\n",
      "notebook\n",
      "network\n",
      "example\n",
      "follow\n",
      "analysis\n",
      "machine\n",
      "code\n",
      "make\n",
      "deep\n",
      "level\n"
     ]
    }
   ],
   "source": [
    "for w in max_words:\n",
    "    print(list(tf_idf_all.vocabulary_.keys())[list(tf_idf_all.vocabulary_.values()).index(w)]) # Prints george"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity of talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# docs_similarity is a [S,S] matrix, where 'S' is a talk\n",
    "# the higher docs_similarity[i,j] indicates the more similar between talk[i] and talk[j]\n",
    "docs_similarity = docs_normed.dot(docs_normed.T)\n",
    "docs_similarity = pd.DataFrame(docs_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 65)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### find top K most similar to each talk\n",
    "def most_similar_talk(s,topk):\n",
    "    # [0] must be itself\n",
    "    similar_ones = s.sort_values(ascending=False)[1:topk+1].index.values\n",
    "    return pd.Series(similar_ones,index = [\"similar#{}\".format(i) for i in range(1,topk+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top10 = docs_similarity.apply(most_similar_talk,topk=10,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar#1</th>\n",
       "      <th>similar#2</th>\n",
       "      <th>similar#3</th>\n",
       "      <th>similar#4</th>\n",
       "      <th>similar#5</th>\n",
       "      <th>similar#6</th>\n",
       "      <th>similar#7</th>\n",
       "      <th>similar#8</th>\n",
       "      <th>similar#9</th>\n",
       "      <th>similar#10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   similar#1  similar#2  similar#3  similar#4  similar#5  similar#6  \\\n",
       "0         51         29         47          9         11         31   \n",
       "1          4          2         41         52         30         50   \n",
       "2          1          4         11         40         56         33   \n",
       "3         31         29         28         27         26         25   \n",
       "4          1         21         11         32          2         20   \n",
       "5         35         24         59          9         34         21   \n",
       "6         50         60         29         13         37         46   \n",
       "7         13         28         39         36         14         26   \n",
       "8         39         52         21         59         56         14   \n",
       "9         29         51         24         31          0          5   \n",
       "\n",
       "   similar#7  similar#8  similar#9  similar#10  \n",
       "0         21         17          6          52  \n",
       "1         58         11         37          21  \n",
       "2         32         22         18           8  \n",
       "3         24         23         22          21  \n",
       "4         30         50         22           8  \n",
       "5         28         29         48           1  \n",
       "6         24         28         36          15  \n",
       "7          4         54         37           1  \n",
       "8         16          1         11          58  \n",
       "9         47         37         52          21  "
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Using CNTK's Python Interface for Deep Learning\n",
      "1. D‚Äôoh! Unevenly spaced time series analysis of The Simpsons in Pandas\n",
      "2. From Novice to Data Ninja\n",
      "3. So you want to be a Python expert?\n",
      "4. Introduction to data analytics with pandas\n",
      "5. Parallelizing Scientific Python with Dask\n",
      "6. pomegranate: fast and flexible probabilistic modeling in python\n",
      "7. Effective Visual Studio\n",
      "8. Data Visualization and Exploration with Python\n",
      "9. A Quick Primer on TensorFrames: Apache Spark and TensorFlow Together\n",
      "10. Python Web Sraping\n",
      "11. Vocabulary Analysis of Job Descriptions\n",
      "12. Keynote (McKinley & Livestream in Cascade)\n",
      "13. Scalable Data Science in Python and R on Apache Spark\n",
      "14. Using Scattertext and the Python NLP Ecosystem for Text Visualization\n",
      "15. Provenance for Reproducible Data Science\n",
      "16. Monitoring Displacement Crises with Python: A Humanitarian Project by Data for Democracy\n",
      "17. Designing for Guidance in Machine Learning\n",
      "18. Automatic Citation generation with Natural Language Processing\n",
      "19. Practical Optimization for Stats Nerds\n",
      "20. PixieDust - make Jupyter Python Notebooks with Apache Spark Faster, Flexible, and Easier to use\n",
      "21. Pandas, Pipelines, and Custom Transformers\n",
      "22. Scan Statistics with Spark Streaming: Distribution Based Real Time Anomaly Detection\n",
      "23. National Geospatial-Intelligence Agency: Changing the Bureaucrat‚Äôs Mind Toward Data-Driven Decisions\n",
      "24. Unlocking the power of AI: A fundamentally different approach to building intelligent systems\n",
      "25. Keynote (McKinley & Livestream in Cascade)\n",
      "26. Robust Automated Forecasting in Python and R\n",
      "27. In-database Machine Learning with Python in SQL Server - Sponsor Talk\n",
      "28. How to be a 10x Data Scientist\n",
      "29. Code First, Math Later: Learning Neural Nets Through Implementation and Examples\n",
      "30. Python and IoT: From Chips and Bits to Data Science\n",
      "31. Applying the four-step \"Embed, Encode, Attend, Predict\" framework to predict document similarity\n",
      "32. WorldRowing.com: End To End Data Analysis\n",
      "33. How diversity drives excellence in our data-driven tech world\n",
      "34. Big data processing with Apache Beam\n",
      "35. Make it Work, Make it Right, Make it Fast - Debugging and Profiling in Dask\n",
      "36. Python for .NET or .NET for Python\n",
      "37. Moving notebooks into the cloud: challenges and lessons learned\n",
      "38. Chatbots - Past, Present and Future\n",
      "39. Of Mice & Python: Building a Brain Observatory for Visual Behavior\n",
      "40. Batch and Streaming Processing in the World of Data Engineering and Data Science\n",
      "41. Forecasting Time Series Data at scale with the TICK stack\n",
      "42. PyData \"Pub\" Quiz! (Kodiak)\n",
      "43. Keynote (McKinley & Livestream in Cascade)\n",
      "44. Sirbarksalot: Bark Detection in Python\n",
      "45. Mosaicking the Earth every day\n",
      "46. High Fidelity Web Crawling in Python\n",
      "47. Medical image processing using Microsoft Deep Learning framework (CNTK)\n",
      "48. Building a community fountain around your data stream\n",
      "49. High-Performance Distributed Tensorflow: Request Batching and Model Post-Processing Optimizations\n",
      "50. Implementing and Training Predictive Customer Lifetime Value Models in Python\n",
      "51. Learn to be a painter using Neural Style Painting\n",
      "52. bqplot - Interactive Data Visualization in Jupyter\n",
      "53. Making packages and packaging \"just work\"\n",
      "54. Beginning Julia: Language and Landscape\n",
      "55. Scaling Scikit-Learn\n",
      "56. Interactive Data Analysis: Visualization and Beyond (McKinley & Livestream in Cascade)\n",
      "57. JupyterLab+Real Time Collaboration\n",
      "58. We came, we saw, we hacked. How to win a Big Data hackathon\n",
      "59. Bokeh and Friends\n",
      "60. Machine Learning Infrastructure at Stripe: Bridging from Python -> JVM\n",
      "61. BrainDrain: Using Machine Learning and Brain Waves to Detect Errors in Human Problem Solving\n",
      "62. Writing a Book in Jupyter Notebooks\n",
      "63. Upgrading Legacy Projects: Lessons Learned\n",
      "64. Robust Algorithms for Machine Learning\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(titles):\n",
    "    print(\"{}. {}\".format(i, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get talk title given the index number in matrix\n",
    "def get_sims(topk, titles, talk_no):\n",
    "    row = topk.loc[talk_no, :].copy()\n",
    "    talk_zero = titles[talk_no]\n",
    "    talks = np.array(titles)[row]\n",
    "    print(\"Talk:\", talk_zero)\n",
    "    print(\"-\"*60)\n",
    "    for i,t in enumerate(talks):\n",
    "        print(\"{}. {}\".format(row[i], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk: Using CNTK's Python Interface for Deep Learning\n",
      "------------------------------------------------------------\n",
      "51. Learn to be a painter using Neural Style Painting\n",
      "29. Code First, Math Later: Learning Neural Nets Through Implementation and Examples\n",
      "47. Medical image processing using Microsoft Deep Learning framework (CNTK)\n",
      "9. A Quick Primer on TensorFrames: Apache Spark and TensorFlow Together\n",
      "11. Vocabulary Analysis of Job Descriptions\n",
      "31. Applying the four-step \"Embed, Encode, Attend, Predict\" framework to predict document similarity\n",
      "21. Pandas, Pipelines, and Custom Transformers\n",
      "17. Designing for Guidance in Machine Learning\n",
      "6. pomegranate: fast and flexible probabilistic modeling in python\n",
      "52. bqplot - Interactive Data Visualization in Jupyter\n"
     ]
    }
   ],
   "source": [
    "get_sims(top10, titles, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk: Robust Automated Forecasting in Python and R\n",
      "------------------------------------------------------------\n",
      "41. Forecasting Time Series Data at scale with the TICK stack\n",
      "1. D‚Äôoh! Unevenly spaced time series analysis of The Simpsons in Pandas\n",
      "13. Scalable Data Science in Python and R on Apache Spark\n",
      "47. Medical image processing using Microsoft Deep Learning framework (CNTK)\n",
      "7. Effective Visual Studio\n",
      "28. How to be a 10x Data Scientist\n",
      "35. Make it Work, Make it Right, Make it Fast - Debugging and Profiling in Dask\n",
      "11. Vocabulary Analysis of Job Descriptions\n",
      "36. Python for .NET or .NET for Python\n",
      "50. Implementing and Training Predictive Customer Lifetime Value Models in Python\n"
     ]
    }
   ],
   "source": [
    "get_sims(top10, titles, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Talk: Python for .NET or .NET for Python\n",
      "------------------------------------------------------------\n",
      "20. PixieDust - make Jupyter Python Notebooks with Apache Spark Faster, Flexible, and Easier to use\n",
      "37. Moving notebooks into the cloud: challenges and lessons learned\n",
      "52. bqplot - Interactive Data Visualization in Jupyter\n",
      "53. Making packages and packaging \"just work\"\n",
      "7. Effective Visual Studio\n",
      "1. D‚Äôoh! Unevenly spaced time series analysis of The Simpsons in Pandas\n",
      "6. pomegranate: fast and flexible probabilistic modeling in python\n",
      "27. In-database Machine Learning with Python in SQL Server - Sponsor Talk\n",
      "62. Writing a Book in Jupyter Notebooks\n",
      "47. Medical image processing using Microsoft Deep Learning framework (CNTK)\n"
     ]
    }
   ],
   "source": [
    "get_sims(top10, titles, 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "+ Use __\"day and time\"__ as constraints to create schedule\n",
    "+ Use additional information such as __'description', 'speaker bio'__\n",
    "+ Create __semantic embeddings (word2vec, doc2vec, lda2vec)__ and cluster talks\n",
    "+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
